%%%%%%%%%%%%%%%%%%%%%%% file template.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is a general template file for the LaTeX package SVJour3
% for Springer journals.          Springer Heidelberg 2010/09/16
%
% Copy it to a new file with a new name and use it as the basis
% for your article. Delete % signs as needed.
%
% This template includes a few options for different layouts and
% content for various journals. Please consult a previous issue of
% your journal as needed.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% First comes an example EPS file -- just ignore it and
% proceed on the \documentclass line
% your LaTeX will extract the file if required
\begin{filecontents*}{example.eps}
%!PS-Adobe-3.0 EPSF-3.0
%%BoundingBox: 19 19 221 221
%%CreationDate: Mon Sep 29 1997
%%Creator: programmed by hand (JK)
%%EndComments
gsave
newpath
  20 20 moveto
  20 220 lineto
  220 220 lineto
  220 20 lineto
closepath
2 setlinewidth
gsave
  .4 setgray fill
grestore
stroke
grestore
\end{filecontents*}
%
\RequirePackage{fix-cm}
%
\documentclass{svjour3}                     % onecolumn (standard format)
%\documentclass[smallcondensed]{svjour3}     % onecolumn (ditto)
% \documentclass[smallextended]{svjour3}       % onecolumn (second format)
%\documentclass[twocolumn]{svjour3}          % twocolumn
%
\smartqed  % flush right qed marks, e.g. at end of proof
%
\usepackage{graphicx}
%
% \usepackage{mathptmx}      % use Times fonts if available on your TeX system
%
% insert here the call for the packages your document requires
%\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{subfig}

% etc.
%
% please place your own definitions here and don't use \def but
% \newcommand{}{}
%
% Insert the name of "your journal" with
% \journalname{myjournal}
%
\begin{document}

\title{Investigating exploration for deep reinforcement learning of concentric tube robot control
\thanks{This work is supported by the Wellcome/EPSRC Centre for Interventional and Surgical Sciences (WEISS) (203145Z/16/Z). The authors acknowledge the use of the UCL Myriad High Throughput Computing Facility (Myriad@UCL), and associated support services, in the completion of this work.}
}

%\titlerunning{Short form of title}        % if too long for running head

\author{Keshav Iyengar \and
        George Dwyer \and Danail Stoyanov
}

%\authorrunning{Short form of author list} % if too long for running head

\institute{K. Iyengar \at
              Gower St, Bloomsbury, London WC1E 6BT\\
              \email{keshav.iyengar@ucl.ac.uk} %  \\
%             \emph{Present address:} of F. Author  %  if needed
}

\date{Received: date / Accepted: date}
% The correct dates will be entered by the editor


\maketitle

\begin{abstract}
\leavevmode\newline
\textbf{Purpose} Concentric tube robots are composed of multiple concentric, pre-curved, super-elastic telescopic tubes that are compliant and have a small diameter footprint ideal for minimally invasive surgery. The overall shape can be manipulated by rotating and extending the tubes relative to each other. The interactions between tubes is complex and closed form inverse kinematics are difficult to formulate past trivial tube architectures. A model-free, data driven approach (DDA) such as reinforcement learning would be useful to form the inverse kinematics solution.
\newline
\textbf{Method} Reinforcement learning has a key trade-off problem, exploration versus exploitation and balancing this trade-off is an important step in converging to a control policy. To this end, this work investigates various exploration strategies for deep deterministic policy gradient with hindsight experience replay with a dominant stiffness concentric tube robot simulation environment. The aim of the study is to determine which, if any, exploration strategies are scalable with respect to number of tubes, an indicator of generality.
\newline
\textbf{Results} The study determines that a zero mean single variate Gaussian noise applied to actions does not explore the workspace in extended joint states. Zero mean multivariate Gaussian of various forms performs much better in exploration of the workspace. Various tube configurations of curvature and relative lengths are tested with the trained model of a given to demonstrate adaptability.
\newline
\textbf{Conclusion} In future works, the found noise process can be applied to variable curvature concentric tube robot models.
\keywords{deep reinforcement learning \and inverse kinematics \and model-free \and policy gradient \and concentric tube robots}
\end{abstract}

\section{Introduction}
\label{intro}
Minimally invasive surgery (MIS) encompasses are large group of techniques that minimize the incision point size in order to reduce healing time and risk of infection. Robotics incorporated with MIS aids the surgeon with precision, vision and ergonomics where traditional MIS is technically challenging because of disassociation of vision and ergonomic factors like monitor placement and operation table height. In some interventions, like those performed with robotic catheters and endoscopes, the robotic devices used are passive along the length and rely on anatomical structures to guide their movement. Any compliance added to the device to limit contact forces with these structures often has the trade-off of reduced tip stiffness detrimental to certain surgical tasks. A greater risk is relying on anatomical structures for guidance can cause damage to surrounding sensitive healthy tissue. Continuum robots are type of robot that have a large number of degrees of freedom distributed along the length of the the robot allowing for tentacle-like motion, advantageous to avoid sensitive structures in the body. Concentric tube robots are a sub-type of continuum robots that use neighboring tube interactions of bending and twisting when rotated and translated to form curvilinear paths. These paths can avoid anatomical structures, be compliant and still offer some dexterity at the tip.
\begin{figure}
  \includegraphics[scale=0.15]{images/ctr-collage.jpg}
\caption{Curvilinear path of two tube concentric tube robot.}
\label{fig:1}
\end{figure}
Common modelling approaches of concentric tube robots are based on special Coserat rods for each tube undergoing bending and torsion that lead to no analytical solution for robots consisting of two tubes or more or for pre-curvature that varies with length \cite{Dupont2010}, \cite{Rucker2010}. Additional factors like friction and tube tolerances have have been investigated \cite{Lock2011} but are difficult to integrate because of the large computational load for modelling efforts. Inverse kinematics strategies applied are common approaches like numerical root finding \cite{Burgner2014} or differential inverse kinematics \cite{Dupont2010}. These strategies are slow and do not guarantee convergence with Coserat rod modelling. A model-free data driven approach (DDA) would be beneficial because of accuracy in real scenarios compared to current model-based inverse kinematics strategies. Furthermore, unlike neural network approaches that have been proposed \cite{Grassmann2018}, reinforcement learning can be trained in successively complex environments and eventually to a real environment by combining training parameters as shown in the Sim2Real \cite{OpenAI2018}. Reinforcement learning is then data efficient, if the cost of collecting real life data is high.

To our knowledge there have been no previous work using reinforcement learning for concentric tube robots. However, two DDA approaches have been proposed. The first approach \cite{Bergeles2015} uses simulated data to train a multi-layer perceptron (MLP) network for inverse kinematics of a 3 tube robot with one variable curvature section. The rotation configuration space is split into four quadrants resulting in an output of a single extension joint value per tube and 4 rotation joint values per tube. The correct joint tuple is selected by examining the least forward kinematics tip error. To avoid bias during training, extension values less than 30\% of the maximum extension value are ignored. The simulation accuracy results demonstrate Cartesian error is below 0.8mm running at 50 Hz in Matlab. The second approach \cite{Grassmann2018} also uses an MLP framework for inverse kinematics and focuses on hardware with a contribution with a novel joint space representation. The representation follows trigonometric joint representation used in other work with adaptions for concentric tube robots. The work defines a cylindrical form $\gamma_i$,
\begin{equation}
\gamma_i = \{ \gamma_{1,i}, \gamma_{2,i}, \gamma_{3,i} \} = \{ \cos(\alpha_i), \sin(\alpha_i), \beta_i \},\label{eqn:1}
\end{equation}
which describes the $i$ tube as a triplet. The rotatory joint $\alpha_i$ can be retrieved by
\begin{equation}
\alpha_i = \textnormal{atan2} \{ \gamma_{2,i}, \gamma_{1,i} \}. \label{eqn:2}
\end{equation}
The extension joint $\beta_i$ can be retrieved directly and has constraints
\begin{align}
0 \geq \beta_n \geq & \dots \geq \beta_2 \geq \beta_1, \label{eqn:3} \\
0 \leq L_n + \beta_n \leq & \dots \leq L_2 + \beta_2 \leq L_1 + \beta_1, \label{eqn:4}
\end{align}
where $n$ is the number of tubes. Another study \cite{Grassmann2019}, investigating various joint space representations confirmed that the cylindrical representation performs much better for MLP frameworks as compared a simple rotation and extension form. Hardware training and evaluation was done with a 3 tube concentric tube robot, the actuation error was $4.0$mm in translation and $8.3 ^{\circ}$ with 60,000 training samples. The cylindrical form and extension constraints are directly used as the joint representation for the reinforcement learning strategy applied in this work.

A major challenge of model-free reinforcement learning in continuous state and action spaces is exploration \cite{Nair2018}. An advantage of using an off-policy algorithm like deep deterministic policy gradient is the learned policy does not have to be the one used for training. In the first continuous control work \cite{Lillicrap2015}, Ornstein-Uhlenbeck was the noise process used for continuous problems but alternatives strategies have been found. The three main exploration methods are applied to model-free reinforcement learning are studied and results presented in this work. The exploration methods are Gaussian noise process, Ornstein-Uhlenbeck noise process and parameter noise. Applying the same exploration to different number of tubes is an indication that the policy can be generalized. Therefore, a focus is placed on finding exploration parameters that work from two to four tube dominant stiffness concentric tube robots.
\section{Methods}
Following the standard paradigm of reinforcement learning the inverse kinematics problem is formulated as a Markov Decision Process (MDP). The action, state and reward of the MDP model is detailed as follows.

\paragraph{State.} The state is a combination the cylindrical representation defined in equation \ref{eqn:1}, the current Cartesian end-effector position $g$ and the desired Cartesian end-effector position $\hat{g}$,
\begin{equation}
s = \left( \gamma_1, \gamma_2, \dots, \gamma_n, g, \hat{g} \right). \label{eqn:5}
\end{equation}

\paragraph{Action.} The action is a change in extension and rotation at one timestep with separate limits for rotation and extension. In DDPG, the agent can select any value in the continuous range in the limit interval.
\begin{equation}
a = \left( \Delta \alpha_1, \Delta \beta_1, \Delta \alpha_2, \Delta \beta_2,  \dots, \Delta \alpha_n, \Delta \beta_n \right). \label{eqn:6}
\end{equation}

\paragraph{Reward.} The reward is the scalar value returned by the environment as feedback to the agent from the chosen action at the current timestep. In \cite{andrychowicz2017hindsight} described how sparse rewards are more effective than dense rewards when using hindsight experience replay for continuous action environment. Moreover, dense rewards are difficult to shape to push the agent towards a desired behaviour. The error is
\begin{equation}
e = \sqrt{(g_x - \hat{g}_x)^2 + (g_y - \hat{g}_y)^2 + (g_z - \hat{g}_z)^2}. \label{eqn:7}
\end{equation}

The reward function can then be defined as
\begin{equation}
r =
  \left\{
    \begin{array}{l}
      \hphantom{-}0, \quad e \leq \delta \\
      -1, \quad \textnormal{otherwise},
    \end{array}
  \right. \label{eqn:8}
\end{equation}

where $\delta$ is the goal tolerance. The tolerance used in this work is 1 mm. An episode consists of a certain number of timesteps for the agent to interact with the environment, before a reset is initiated or the desired goal has been reached. The reward function is calculated at each timestep and is cumulative through the episode, therefore the agent is incentivized to use the fewest timesteps to the desired goal.

\paragraph{Policy Learning.} A multi-layer perceptron network (MLP) is used to model the policy network. The network has inputs size that of the environment state dimension and outputs size that of environment action dimension. With a MDP defined, any standard reinforcement learning method can be applied to learn a policy. The chosen method was deep deterministic policy gradient (DDPG) \cite{Lillicrap2015} with hindsight experience replay (HER) \cite{andrychowicz2017hindsight}. The reason we chose this method is two fold. First, because our environment is a continuous state and action problem and require an off-policy method, DDPG was chosen. Second, because successes in training are sparse, HER was chosen to add successful samples by appending saved episode trajectories.

\subsection{Simulation}
The kinematic model of the concentric tube robot is the dominant stiffness model \cite{Dupont2010}. For tube $i$, rotation, $\alpha_i$, is relative to the base of the tube, $\kappa_i$ is the constant curvature and $L_i + \beta_i$ is the extension length. A transformation representing the curvature for a tube is defined as

\begin{equation}\label{curvature-transformation}
\begin{aligned}
\textbf{T}_{curv,i} &=
\left[\begin{matrix}
  c^2_\alpha (c_{\kappa  \left(L+\beta\right)} - 1) + 1 & s_\alpha c_\alpha (c_{\kappa  \left(L+\beta\right)} - 1) \\
  s_\alpha c_\alpha (c_{\kappa  \left(L+\beta\right)} - 1) & c^2_\alpha (1 - c_{\kappa  \left(L+\beta\right)}) + c_{\kappa  \left(L+\beta\right)} \\
  c_{\phi} s_{\kappa  \left(L+\beta\right)} & s_{\phi} s_{\kappa  \left(L+\beta\right)} \\
  0 & 0
\end{matrix}\right.\\
&\qquad\qquad
\left.\begin{matrix}
  - c_\alpha s_{\kappa  \left(L+\beta\right)}  & \frac{L+\beta}{k} c_\alpha (c_{\kappa \left(L+\beta\right)} - 1) \\
  - s_\phi c_{\kappa \left(L+\beta\right)} & \frac{1}{k} s_\phi (c_{\kappa  \left(L+\beta\right)} - 1) \\
  c_{\kappa s} & \frac{1}{k} s_{\kappa  \left(L+\beta\right)} \\
  0 & 1
\end{matrix}\right],
\end{aligned}
\end{equation}
For the end effector of a robot of $n$ tubes, the forward kinematics can be defined as
\begin{equation}
\textbf{T}_{ee} = \prod^{n}_{i} \textbf{T}_{curv,i}
\end{equation}

\paragraph{Desired goal sampling.} When sampling desired goals from simulation for reinforcement learning, the Cartesian space sampling is not uniform. The desired goals are chosen to be achievable goals by the robot, and therefore, must satisfy the constraints found in equation \ref{eqn:3} and \ref{eqn:4}. Because there are no such constraints on $\alpha$, rotation sampling is uniform. For $\beta$, the extensions must satisfy conditions and all though initially sampled uniformly, once constraints are considered there is a bias in Cartesian desired goal points.
\begin{figure}%
    \centering
    \subfloat[3 tube robot]{{\includegraphics[width=5cm]{images/3-tube-x-y-z.png}}}%
    \qquad
    \subfloat[4 tube robot]{{\includegraphics[width=5cm]{images/4-tube-x-y-z.png}}}%
    \caption{2 Figures side by side}%
    \label{fig:example}%
\end{figure}

\subsection{Exploration}
There are three main exploration techniques found in model-free reinforcement learning. All three techniques add a noise to the action during training.

\paragraph{Zero-mean multivariate Gaussian noise}
Given a standard deviation, each action during training is perturbed by sampling a value from a zero-mean Gaussian distribution and arithmetically adding it to the selected action by the policy.
\begin{equation}
    a = \mu(s) + \mathcal{N} (0, \sigma^2 I)
\end{equation}
\begin{equation}
    a = \mu(s) + \mathcal{N} (\Vec{0}, \Sigma)
\end{equation}
With a single standard deviation multivariate Gaussian, a single standard deviation value is used to perturb action states of both extension and rotation. However, with extension and rotation having different units, it is a non-ideal noise process. With a multiple standard deviation multivariate Gaussian, each action index can have an independent standard deviation. Extension joints and rotation joints can have independent standard deviations, furthermore, different tubes can have different extension standard deviations to push for tube extension in inner tubes.
\paragraph{Ornstein Uhlenbeck noise}
Ornstein-Uhlenbeck (OU) noise process was the original noise process in the DDPG work  \cite{Lillicrap2015}. The noise is temporally correlated allowing to set a long-term mean $\mu$. The process moves towards $\mu$ with a given standard deviation $\sigma$ at a rate $\theta$ and current value $x_t$ over timesteps of the episode and is reset with an episode termination.
\begin{equation}
    a = \mu(s) + OU \left( \Vec{x_{t}}, \theta, \Vec{\mu}, \Sigma \right)
\end{equation}
\paragraph{Parameter noise}
Parameter noise adds noise directly to the policy network weights during training for exploration \cite{plappert2017parameter}. Zero mean multivariate Gaussian distribution of size equal to the parameter vector of the policy network is sampled and used to perturb the policy weights directly.
\begin{equation}
    a = \mu(s | \theta^{\mu} + \mathcal{N} (\Vec{0}, \sigma^2 I)) 
\end{equation}
Adding noise directly to the agent's parameters allows for more consistent exploration across timesteps, whereas exploration added to actions leads to unpredictable exploration which is not correlated to the agent's parameters \cite{plappert2017parameter}.

The study proposed is to investigate these exploration strategies in terms of accuracy and scalability with respect to number of tubes. The proposed method of investigation is the following. First, for base hyperparameter values of DDPG and HER, a search is performed for in a single tube environment. The base hyperparameters are extended two tubes, three tubes and four tubes. Next a hyperparameter search for only the standard deviation of a multivariate Gaussian of the final tube extension index is performed for two tubes. This result is used for three and four tubes. Next, a hyperparameter search is done for a parameter noise standard deviation for the two tube environment with the result applied to three and four tube training. 

\subsection{Computing Hardware}
For training, a server cluster with Intel Xeon Gold 6140 18C 140W 2.3GHz was used with 19 parallel workers created as described in \cite{OpenAI2018}. With independent workers, exploration improves.

\section{Experiments}

\section{Results}
To find base hyperparameters for DDPG and HER, a hyperparameter search was performed on a one tube robot. The search was with a random sampler and median pruner with 1000 trials and 20000 episodes per trial for approximately 2 million steps in total. The cost function was negative mean cumulative reward per episode. The resulting hyperparameters values had a cost of 50.6 and can be found in table \ref{tab:2}.

To test scalability, the base hyperparameters are applied to 2,3 and 4 tube environments. The success rate is quite low for all environments and visualizing the point clouds of the achieved goals used in training shows a bias to retraction.
\begin{itemize}
    \item Show data of 0.35 Gaussian noise in a cool way
\end{itemize}
To test if independent standard deviations of rotation and extension would improve accuracy and exploration, a hyperparameter search was done for rotation and extension standard deviations of a 2 tube environment. Similar to the base hyperparameter search, 1000 trials and 20000 episodes per trial with a random sampler and median pruner were selected and a value of 0.025 for rotation and 0.00065 for extension with a cost of 60.2. The resulting trained model of a two tube environment had much a higher success rate and lower errors. The point clouds also show increased exploration with achieved goal points further out in extension of the tubes.
\begin{itemize}
    \item Show data of 0.025, 0.00065 Gaussian noise in a cool way
\end{itemize}
% For tables use
\begin{table}
% table caption is above the table
\caption{Hyperparameter Descriptions}
\label{tab:1}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{lll}
\hline\noalign{\smallskip}
Hyperparameter & Description\\
\noalign{\smallskip}\hline\noalign{\smallskip}
timesteps & Total number of timesteps for training.\\
sampled\_goals & The number of artificial transitions to \\
& generate for each actual transition. \\
buffer\_size & The max number of transitions to store,\\
&  size of the replay buffer.\\
batch\_size &  The size of the batch for learning the policy.\\
gamma & Discount factor.\\
tau & Soft update coefficient for target network.\\
random\_exploration & Probability of taking a random action.\\
noise\_type & Type of noise to apply.\\
noise\_std & Standard deviation of specified noise type.\\
actor\_learning\_rate & The actor learning rate.\\
critic\_learning\_rate & The critic learning rate.\\
actor and critic network & Actor and critic network architecture.\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

% For tables use
\begin{table}
% table caption is above the table
\caption{Static hyperparameters}
\label{tab:2} 
% For LaTeX tables use
\begin{tabular}{lllll}
\hline\noalign{\smallskip}
Hyperparameter  & Tube 1 \\
\noalign{\smallskip}\hline\noalign{\smallskip}
sampled\_goals & 4 \\
buffer\_size & 10000 \\
batch\_size & 256 \\
gamma & 0.95 \\
tau & 0.001 \\
random\_exploration & 0.294 \\
noise\_type & normal \\
noise\_std & 0.35 \\
actor\_learning\_rate & 0.001 \\
critic\_learning\_rate & 0.001 \\
actor and critic network & [128, 128, 128] \\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

% For tables use
\begin{table}
% table caption is above the table
\caption{Concentric tube robot environment parameter description}
\label{tab:3}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{lll}
\hline\noalign{\smallskip}
Parameter & Description & Units\\
\noalign{\smallskip}\hline\noalign{\smallskip}
k & Curvature of each tube. & m$^{-1}$ \\
l\_tip & Length of end-effector straight section. & m \\
tube\_length & Length of each tube. & m\\
tube\_rotation\_limits & Range of orientation for rotation & $^\circ$ \\
action\_length\_limit & Largest change in length in single timestep. & m \\
action\_rotation\_limit & Largest change in length in single timestep. & $^\circ$ \\
goal\_tolerance & Tolerance to desired goal for a successful episode. & m \\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

% For tables use
\begin{table}
% table caption is above the table
\caption{Concentric tube robot environment parameters}
\label{tab:4}       % Give a unique label
% For LaTeX tables use
\begin{tabular}{lllll}
\hline\noalign{\smallskip}
Parameter & 1-Tube & 2-Tube & 3-Tube & 4-Tube\\
\noalign{\smallskip}\hline\noalign{\smallskip}
k & 25.0 & 25.0, 20.0 & 15.0 & 10.0\\
l\_tip  & 0.007 & 0.007 & 0.007 & 0.007\\
tube\_length & 0.10 & 0.15, & 0.12, 0.07, & 0.15, 0.10, \\
& & 0.10 & 0.04 & 0.05, 0.025 \\
tube\_rotation\_limits  & -180$^\circ$,  & -180$^\circ$, & -180$^\circ$, & -180$^\circ$,\\
& 180$^\circ$ & 180$^\circ$ & 180$^\circ$ & 180$^\circ$\\
action\_length\_limit & -0.0001, 0.0001 & -0.0001, & -0.0001, & -0.0001, \\
& 0.0001 & 0.0001 & 0.0001 & 0.0001 \\
goal\_tolerance & 0.001 & 0.001 & 0.001 & 0.001\\

\noalign{\smallskip}\hline
\end{tabular}
\end{table}

\begin{itemize}
    \item Show table of static hyperparameters
    \item Table of dynamic hyperparameters
    \item Tests:
    \begin{itemize}
        \item Multivariate Gaussian noise of different extension stds on different tubes
        \item Hyperparameter search for parameter noise value on different tubes
        \item OU noise with similar Gaussian noise stds on different tubes
        \item Take best noise on each tube, do a online training test
    \end{itemize}
    \item Hopefully conclusion is either OU or Gaussian works best and can be used even online.
    \item Future work is adapting best noise parameter for realisitic models since exploration is somewhat understood.

\end{itemize}

\section{Conclusion}

%\begin{acknowledgements}
%If you'd like to thank anyone, place your comments here
%and remove the percent signs.
%\end{acknowledgements}


% Authors must disclose all relationships or interests that 
% could have direct or potential influence or impart bias on 
% the work: 
%
% \section*{Conflict of interest}
%
% The authors declare that they have no conflict of interest.


% BibTeX users please use one of
%\bibliographystyle{spbasic}      % basic style, author-year citations
%\bibliographystyle{spmpsci}      % mathematics and physical sciences
\bibliographystyle{spphys}       % APS-like style for physics
\bibliography{refs}   % name your BibTeX data base

% Non-BibTeX users please use
%\begin{thebibliography}{}
%
% and use \bibitem to create references. Consult the Instructions
% for authors for reference list style.
%
% \bibitem{RefJ}
% Format for Journal Reference
% Author, Article title, Journal, Volume, page numbers (year)
% Format for books
% \bibitem{RefB}
% Author, Book title, page numbers. Publisher, place (year)
% etc
% \end{thebibliography}

\end{document}
% end of file template.tex

